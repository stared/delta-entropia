\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T4]{fontenc}
\usepackage[polish]{babel}
%\usepackage {polski}
\let\lll\undefined
\usepackage{amssymb,amsmath,amsthm}
\marginparwidth=5.2cm \hoffset=2.5cm \textwidth=12.5cm
\textheight=25.2cm \reversemarginpar \voffset=-1in
%\addtolength{\textheight}{2in}

% Moje pliki latexowe artykułów, które napisałem mają mniej więcej 1100 - 1600 słów
% i to jest trochę ponad 2 strony. A więc celuj w 1000 słów (lub 500 - na 1 stronę), a jak przekroczysz
% to nic się nie stanie. Objętość tekstu nie jest taka ważna, ważniejsza jest treść ;).

\begin{document}
%tytuł
\noindent\textbf{\LARGE Informacja czy chaos}

\medskip
%autor
\noindent\textit{\Large Piotr Migdał} \marginpar{\footnotesize
*dr z ICFO -- The Institute of Photonic Sciences, Castelldefels (Barcelona)}

\medskip

O ile entropia jest używana w języku potocznym jako chaos i nieuporządkowanie, to sama wielkość jest ściśle określonym pojęciem
%
\begin{align}
    H = -\sum_{i=1}^{n} p_i \log(p_i),
\end{align}
%
gdzie $\{p_1, \ldots, p_n\}$ to pewien rozkład prawdopodobieństwa. Będziemy używać podstawy logarytmu $2$, co odpowiada mierzeniu entropii w \emph{bitach}. 

    \marginpar{\footnotesize Korzystatnie z innej podstawy logarytmu, np. $e=2.718\ldots$ przemnoży wynik przez stałą, a zatem odpowiada tylko zmianie jednostek: $ln(x)=ln(x)/ln(2)$.}

Kodowanie 


Informacja wzajemna

Inne miary 

Temperatura




Gramy w 20 pytań - 


 \marginpar{\footnotesize Dygresja}


 Lektury:


\end{document}
