\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\DeclareUnicodeCharacter{00A0}{~}  % or not it by utf8x
\usepackage[T4]{fontenc}
\usepackage[polish]{babel}
%\usepackage {polski}
\let\lll\undefined
\usepackage{amssymb,amsmath,amsthm}
\usepackage{hyperref}
\marginparwidth=5.2cm \hoffset=2.5cm \textwidth=12.5cm
\textheight=25.2cm \reversemarginpar \voffset=-1in
%\addtolength{\textheight}{2in}

% Moje pliki latexowe artykułów, które napisałem mają mniej więcej 1100 - 1600 słów
% i to jest trochę ponad 2 strony. A więc celuj w 1000 słów (lub 500 - na 1 stronę), a jak przekroczysz
% to nic się nie stanie. Objętość tekstu nie jest taka ważna, ważniejsza jest treść ;).

\begin{document}
%tytuł
\noindent\textbf{\LARGE Prawdopodobieństwo a informacja}

\medskip
%autor
\noindent\textit{\Large Piotr Migdał} \marginpar{\footnotesize
*dr z ICFO--The Institute of Photonic Sciences, Castelldefels (Barcelona)}

\medskip

O ile entropia jest używana w języku potocznym jako chaos i nieuporządkowanie, to sama wielkość jest ściśle określonym pojęciem --- \emph{informacją Shannona}:
%
    \marginpar{\footnotesize Cześciej jest pisane w sumie $-p_i \log(p_i)$ --- równoważnie ale, moim zdaniem, mniej dydaktycznie.}
%
\begin{align}
    H = \sum_{i=1}^{n} p_i \log \left(\tfrac{1}{p_i} \right),
\end{align}
%
    \marginpar{\footnotesize Korzystatnie z innej podstawy logarytmu, np. $e=2.718\ldots$ przemnoży wynik przez stałą, a zatem odpowiada tylko zmianie jednostek: $\ln(x)=\ln(2)\log_2(x)$.}
%
gdzie $\{p_1, \ldots, p_n\}$ to pewien rozkład prawdopodobieństwa. Będziemy używać podstawy logarytmu $2$, co odpowiada mierzeniu entropii w \emph{bitach}. 

Gdy nasz rozkład składa się z jednej możliwości, entropia wynosi $0 \log(1) = 0$ bitów --- wszak nie ma tu miejsca na losowość.
Gdy rzucamy uczciwą monetą, entropia to $\tfrac{1}{2} \log(2) + \tfrac{1}{2} \log(2) = 1$ bit.
Entropia jest największa, gdy dla ustalonej liczby zdarzeń wszystkie są równo prawdopodobne --- wtedy entropia to $\log(n)$.
Dla kostki z $6$ ścianami to $\log(6)\approx 2.6$. 

    % \marginpar{\footnotesize Z matematycznego punktu widzenia $\lim_{p \to 0} p \log(p) = 0$, z fizycznego --- nie chcemy by dodanie nierealistycznej opcji (tj. z zerowym prawdopodobieństwem) zmieniało wynik.}

Dlaczego potrzebujemy w tym wzorze logarytmu? 
W skrócie, by ``entropia zachowała się jak informacja''. Gdy mamy dwa niezależne zdarzenia, chcemy by ich entropia była sumą entropii składników.

    \marginpar{\footnotesize Czytelnikowi, który zastanawia się czy owe dane są niezależne, polecam zapoznać się z \href{http://blog.okcupid.com/index.php/how-races-and-religions-match-in-online-dating/}{tymi danymi}.}
%
Powedzmy, że chcemy dowiedzieć się jaki jest znak zodiaku naszego obiektu westchnień oraz czy nas kocha. Nie powinno grać roli czy dowiemy się na raz jednej informacji czy po kawałku.
Oznaczymy prawdopodobieństwa znaków zodiaku jako $\{p_1, \ldots, p_12 \}$ oraz uczucie od nas jako $\{q_1, q_2\}$.
%
\begin{align}
    \sum_{i=1}^{12} \sum_{j=1}^2 p_i q_j \log(1/(p_i q_j))
    &= \sum_{i=1}^{12} \sum_{j=1}^2 p_i q_j \left( \log(1/p_i) + \log(1/q_j) \right)\\
    &= \sum_{i=1}^{12} \sum_{j=1}^2 p_i q_j \log(1/p_i)
    -\sum_{i=1}^{12} \sum_{j=1}^2 p_i q_j \log(1/q_j)\\
    &= \sum_{i=1}^{12} p_i \log(1/p_i)
    + \sum_{j=1}^2 q_j \log(1/q_j)
\end{align}


Np. gramy w 20 pytań i chcemy zadać pytanie, z którego odpowiedzi najwięcej wynika. Możemy zapytać ``Czy to jest żywe?'', gdzie mamy z grubsza pół-na-pół szanse, że usłyszymy ``tak'' lub ``nie''.  Zatem zadając takie pytanie, dowiadujemy się
\begin{align}
H(\{ \tfrac{1}{2},\tfrac{1}{2}\}) = \tfrac{1}{2} \log(2) + \tfrac{1}{2} \log(2)  = 1
\end{align}
czyli jeden bit informacji. 

A może warto zadać pierwsze pytanie ``Czy chodzi o krzeszło?''? Wtedy z jednej strony jest mała szansa usłyszenia ``tak'' (powiedzmy, $1/1024$), niemniej gdy takowa odpowiedź padnie dostaniemy sporo informacji  ($-\log(1/1024)=10$). Jednak średnio po uzyskaniu odpowiedzi dostaniemy
\begin{align}
    H(\{ \tfrac{1}{1024},\tfrac{1023}{1024}\}) = \tfrac{1}{1024} \log(2014) + \tfrac{1023}{1024} \log(\tfrac{1024}{1023}) \approx 0.01
\end{align}
bita, czyli niewiele.


\section{TO DO:}

\begin{itemize}
    \item Prawdopodobieństwo a informacja
    \item Kodowanie
    \item Gramy w 20 pytań!
    \item Temperatura?
\end{itemize}

\section{Kolejne odcinki}

\begin{itemize}
    \item Informacja wzajemna
    \item Inne entropie
\end{itemize}


 \marginpar{\footnotesize  Lektury:}


\end{document}
